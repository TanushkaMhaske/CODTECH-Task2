import numpy as np
import matplotlib.pyplot as plt
import random

class GridWorld:
    def __init__(self, grid_size=(5, 5), start=(0, 0), goal=(4, 4), obstacles=[]):
        self.grid_size = grid_size
        self.start = start
        self.goal = goal
        self.obstacles = obstacles
        self.state = start
        self.action_space = ['up', 'down', 'left', 'right']

    def reset(self):
        self.state = self.start
        return self.state

    def step(self, action):
        row, col = self.state

        if action == 'up':
            row = max(0, row - 1)
        elif action == 'down':
            row = min(self.grid_size[0] - 1, row + 1)
        elif action == 'left':
            col = max(0, col - 1)
        elif action == 'right':
            col = min(self.grid_size[1] - 1, col + 1)

        new_state = (row, col)

        if new_state in self.obstacles:
            new_state = self.state  # stay in place if hitting an obstacle

        self.state = new_state

        if self.state == self.goal:
            reward = 1
            done = True
        else:
            reward = 0
            done = False

        return new_state, reward, done

    def render(self):
        grid = np.zeros(self.grid_size)
        for obs in self.obstacles:
            grid[obs] = -1  # obstacles
        grid[self.goal] = 1  # goal
        grid[self.state] = 2  # agent's position
        print(grid)

class QLearningAgent:
    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.env = env
        self.alpha = alpha  # learning rate
        self.gamma = gamma  # discount factor
        self.epsilon = epsilon  # exploration rate
        self.q_table = {}  # initialize Q-table

    def get_q_value(self, state, action):
        return self.q_table.get((state, action), 0.0)

    def choose_action(self, state):
        if random.uniform(0, 1) < self.epsilon:
            return random.choice(self.env.action_space)  # explore
        else:
            q_values = [self.get_q_value(state, action) for action in self.env.action_space]
            max_q = max(q_values)
            best_actions = [action for action, q_value in zip(self.env.action_space, q_values) if q_value == max_q]
            return random.choice(best_actions)  # exploit

    def update_q_value(self, state, action, reward, next_state):
        max_q_next = max([self.get_q_value(next_state, next_action) for next_action in self.env.action_space])
        current_q = self.get_q_value(state, action)
        new_q = current_q + self.alpha * (reward + self.gamma * max_q_next - current_q)
        self.q_table[(state, action)] = new_q

    def train(self, episodes=1000):
        rewards = []
        for episode in range(episodes):
            state = self.env.reset()
            total_reward = 0
            done = False

            while not done:
                action = self.choose_action(state)
                next_state, reward, done = self.env.step(action)
                self.update_q_value(state, action, reward, next_state)
                state = next_state
                total_reward += reward

            rewards.append(total_reward)

        return rewards

if __name__ == "__main__":
    grid_size = (5, 5)
    start = (0, 0)
    goal = (4, 4)
    obstacles = [(1, 1), (2, 2), (3, 3)]

    env = GridWorld(grid_size, start, goal, obstacles)
    agent = QLearningAgent(env)

    episodes = 1000
    rewards = agent.train(episodes)

    plt.plot(rewards)
    plt.title("Rewards over Episodes")
    plt.xlabel("Episodes")
    plt.ylabel("Reward")
    plt.show()

    state = env.reset()
    done = False
    while not done:
        env.render()
        action = agent.choose_action(state)
        state, _, done = env.step(action)
